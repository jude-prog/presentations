{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a0b943",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding Encoder And Decoder LLMs\n",
    "\n",
    "### In this presentation, I'll talk about:\n",
    "\n",
    "* **Generative AI**\n",
    "\n",
    "* **Self attention**\n",
    "\n",
    "* **Machine Learning architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae75c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative AI\n",
    "It refers to tools that can be used to create new content such as articles or images just like humans can.\n",
    "\n",
    "Generative AI that read and write text are called the Large Language Model(LLMs).\n",
    "\n",
    "A prompt is basically an input to a model.\n",
    "\n",
    "Fundamentally, both encoder- and decoder-style architectures use the same self-attention layers to encode word tokens. However, the main difference is that encoders are designed to learn embeddings that can be used for various predictive modeling tasks such as classification. In contrast, decoders are designed to generate new texts, for example, answering user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d23b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self-Attention\n",
    "This is the ability of a transformer model to attend to different parts of the input sequence when making predictions.\n",
    "\n",
    "We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input’s context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. \n",
    "\n",
    "This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728daea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer Architecture of Gen AI\n",
    "\n",
    "The Transformer architecture is the fundamental building block of all LLMs. It has made it possible for models to generate more accurate and contextually relevant output. \n",
    "\n",
    "It gives it the ability to perform various natural language processing tasks, such as text generation, summarization, and question-answering. LLMs like are opening up new possibilities for communication and human-machine interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11a052",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Input Embeddings\n",
    "\n",
    "The tokens entered by the user are considered inputs for the machine learning models. However, models only understand numbers, not text, so these inputs need to be converted into a numerical format called “input embeddings.” Input embeddings represent words as numbers, which machine learning models can then process. These embeddings are like a dictionary that helps the model understand the meaning of words by placing them in a mathematical space where similar words are located near each other. During training, the model learns how to create these embeddings so that similar vectors represent words with similar meanings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632495b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positional Encoding\n",
    "\n",
    "In natural language processing, the order of words in a sentence is crucial for determining the sentence’s meaning. However, traditional machine learning models, such as neural networks, do not inherently understand the order of inputs. To address this challenge, positional encoding can be used to encode the position of each word in the input sequence as a set of numbers. These numbers can be fed into the Transformer model, along with the input embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5d690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder is part of the neural network that processes the input text and generates a series of hidden states that capture the meaning and context of the text. The encoder in GPT first tokenizes the input text into a sequence of tokens, such as individual words or sub-words. It then applies a series of self-attention layers. Multiple layers of the encoder can be used in the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa90d65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The positionally encoded input representation and the positionally encoded output embeddings go through the decoder. The decoder is part of the model that generates the output sequence based on the encoded input sequence. During training, the decoder learns how to guess the next word by looking at the words before it. Like an encoder, multiple layers of decoders are used in the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da8d43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output Embeddings\n",
    "\n",
    "Models can only understand numbers, not text, like input embeddings. So the output must be changed to a numerical format, known as “output embeddings.” Output embeddings are similar to input embeddings and go through positional encoding, which helps the model understand the order of words in a sentence. The adjustment ultimately improves the model’s overall performance, which is great! Output embeddings are used during both training and inference in GPT. During inference, they generate the output text by mapping the model’s predicted probabilities of each token to the corresponding token in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2ae33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Layer and Softmax\n",
    "\n",
    "After the decoder produces the output embeddings, the linear layer maps them to a higher-dimensional space. This step is necessary to transform the output embeddings into the original input space. Then, we use the softmax function to generate a probability distribution for each output token in the vocabulary, enabling us to generate output tokens with probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2a9f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "presentations",
   "language": "python",
   "name": "presentations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
